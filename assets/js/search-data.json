{
  
    
        "post0": {
            "title": "Tabletoolz",
            "content": "Implementing Tabletoolz for SqlAlchemy . Abstract . Tabletoolz is a python library which provides users with data manipulation approaches used in R’s Dplyr to manipulate tables in SQL and Pyspark. It aims to provide an experience where a user can use R’s Dplyr approach for data manipulation in Python. This paper outlines methods of how this package was implemented accompanied by results of the objective achieved. If a user has used Dplyr, is familiar with its piping capabilities to chain functions together and is looking to work with SQL tables in Python, this is the library to check out! . Background . There are many widely used data manipulation packages today in the data science ecosystem. A few names that come up are Dplyr in R and Pandas in Python. However, the style that we are trying to emulate comes from Dplyr. Dplyr is unique in that it has a pipe operator that enables function chaining. On top of that, it builds on its data vocabulary that makes every sequence of action flow nicely. One library in Python that tries to recreate the Dplyr style of data manipulation using the Python Pandas library is dfply. It uses a decorator-based architecture for the piping functionality with the goal of making it concise and easily extensible. With that said, Tabletoolz borrows decorators and pipes from dfply to be used to extend these functionalities to the library. The library already has the foundations of working with SQL tables built. . Introduction . Data manipulation is the way in which data can be manipulated and changed. It is the part of the data science workflow that stands between sourcing your data and modelling your data. Without processing your data, your data would not necessarily be in the right form to conduct analysis on. There are many ways in which you can perform data manipulation, one of which is by using the grammar of data manipulation. The grammar of data manipulation provides a consistent set of verbs that solves the most common data manipulation problems, popularized by Hadley Wickham, creator of R’s Tidyverse, a collection of packages designed for data science (Wickham, 2014) In Python, there have been efforts made to reproduce this approach in the likes of dfply for Pandas which this project takes inspiration and borrows from. However, we do not yet have such libraries for SQL and PySpark where we can follow these processes of that in the Tidyverse. . With Tabletoolz, we wanted to improve the tools available in python data management. Our objective was to implement SqlAlchemy into the package by writing functions that work with SQL tables using the grammars for data manipulation (Wickham, 2014). Aside from writing functions, good testing practice was done on each function to ensure that these functions perform as expected. Lastly, documentation was written for each function. On top of the main objectives, some of the additional goals are improving proficiency in software engineering by learning how to write functions that solve data manipulation problems. Additionally, I wanted to gain experience in the development cycle of a software project by learning to compile code into a Python package. . Methods . To get started, I had to understand what I was working with. The libraries that were available to me were Python and SqlAlchemy. SqlAlchemy is the Python SQL toolkit that gives application developers the full power and flexibility of SQL. As per the SqlAlchemy documentation, “It provides a full suite of well-known enterprise-level persistence patterns, designed for efficient and high-performing database access, adapted into a simple and Pythonic domain language.” Due to the fact that most of the foundational code was already build, my role was to complete the library with functional functions of data manipulation. Below is a figure describing the process. . . It all starts with an idea of what the function does. Take for example the select function. It chooses variables from the table and keeps only the variables chosen. I then check to see if SqlAlchemy already has that implemented. If it does, then a wrapper function goes on top of the pre-existing function. A wrapper function is composed of a decorator like @pipeable (this gives the function capabilities to pipe like R’s Dplyr) and some code tweaks to the function to make it compatible with the library. After the code works, it goes through testing to see if it really works. Testing involves searching for edge cases to break the function. This ensures that the function is robust and accounts for all foreseeable mistakes that a user makes. If everything works as intended, the code can be documented. The docstring describes what the function does. For every function coded, a docstring is written for it. It describes what the functions does, inputs that go into the functions and outputs, alongside types that should be used. If in an event that the code does not work, I go back to the drawing board and draw up new ideas and go through the process again. . Results . In this section, I will layout the three outcomes achieved in the project. The first “writing functions” is shown with an example of the arrange function from Figure 2. Its usage is to sort a table in either ascending or descending order according to the columns supplied to it. The second objective “writing documentations” (After the def statement in Figure 2) is annotated with “”” strings “”” within its function cell to provide information on how the function work and what the inputs and outputs should be. Additionally, the user is cautioned if there are limits that they should be aware of when using the function. . Functions . Arrange changes the ordering of the rows. | . Helper functions . On top of writing functions, one important aspect I would like to highlight is writing helper functions which provides some useful functionality to complement the main functions within the library. Figure 3 shows examples of helper functions written for the library. The first function intersperse aids the unite function in adding a column separator that unites each column. The following two functions get_onclause_col and table_check are preliminary checks to make sure joins can be performed on tables. . . Writing tests . The third and final objective is to “write tests”. The methods used for testing are asserting whether the results are the same if one were to do it in pandas (asserting data frames), trying inputs of different data types to break the function and raising exceptions if the data type is not compatible with the function. I implemented several scenarios in Figure 4. The first function, test_rename, takes the response function and tests it to see whether its output matches that of the operation if done in the same way using the Python Pandas library. The second function, test_rename_exception, is to test the outcomes of using different types of inputs to see which inputs are not compatible with the function. The inputs that crash the function may have a different data type that is not supported by the function; therefore, we would have to account for these cases during documentation. . . Discussion . After going through the results of the project, I will discuss how the package can be used in a setting where it might be used. For example, we are working with a “Car Options” table that contains information about the options that can be modified onto your car. Say we would like to compute the average of each option. First we would have to import the libraries needed which are TableToolz and SqlAlchemy as shown in Figure 6. . . Next, Figure 7 shows best practices in Tabletoolz by first turning tables into a statement as SQL works with statements. Additionally, a user can print the statement to see what it looks like and return the results of the table by turning the statement into a pandas dataframe. . . Lastly, Figure 8 shows how we would solve a standard data problem in TableToolz. We first define the Mean function then turn the table into a statement, have the variable that we would like to group by and mutate a new variable to compute its average. Then, we can select the columns that we would like to return and finally return its results by turning it into a pandas dataframe. . . Conclusion . In conclusion, I have gone through the goals of the project, methods involved from ideas to coding, documentation and finally testing. All in all, I have written a total of 7 functions for TableToolz Sql which are arrange, drop, rename, unite, left join, inner join, group by. Throughout the course of this project, I have learned a lot about software engineering. The first is going from conceptualizing ideas such as learning about how the functions work and what is it supposed to do then breaking it down into smaller chunks and then translating it into code. I have found that it gave me a better understanding of how it works as a whole. I have also learnt about the importance of testing to make the code more robust and least likely to fail through multiple perspective of test that I have mentioned above. Also the importance of documentations cannot be missed because it provides meaning to the function and how it is used. I wrote documentation for every function in the library. Lastly, on top of the objectives that were achieved, I also learned about structuring directories, cleaning up and organizing files into appropriate folders. . Overall, this capstone project was successful. The objectives met are shown in the results and an example under the discussions section shows a full example of how TableToolz can be incorporated into one’s workflow. In spite of this, there are some limitations to be highlighted, mainly the core SqlAlchemy library where there is not a lot of core data manipulation functions built in compared to libraries like Dplyr. Due to this limitation, a lot of wrangling has to be done in python to find a workaround to implement these functions. However this project can be improved. In the future, refactoring can be done to improve the readability of the code base and more additional functions can be ported into the SQL portion of the library. On top of that, the Pyspark portion remains a work in progress therefore some help could go there. . Acknowledgements . I would like to thank Dr Iverson for giving me an opportunity to contribute to this project. . References . Wickham, Hadley &amp; François, Romain. (2014). dplyr: A Grammar of Data Manipulation. . Dfply https://github.com/kieferk/dfply . SqlAlchemy https://docs.sqlalchemy.org/en/13/core/ . Python https://www.python.org/ . Appendix . Functions . Drop, drops columns from the data frame. . | Group-by allows operations to be performed by “group” . | Rename, changes the name of a column. . | Unite, concatenates columns together. . | Inner join joins tables using the inner join method. . | Left join joins tables using the left join method. . | . Testing . . .",
            "url": "https://dqniellew1.github.io/dqniellew/markdown/paper/data%20manipulation/projects/2020/06/05/Tabletoolz.html",
            "relUrl": "/markdown/paper/data%20manipulation/projects/2020/06/05/Tabletoolz.html",
            "date": " • Jun 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Tabletoolz demonstration",
            "content": "About . In my last post, I wrote about my motivations, methods and results on Tabletoolz. This post will give a demonstration of how the Tabletoolz library can be used. . The libraries we will be working with are SqlAlchemy and TableToolz. . # imports import tabletoolz.sql as tbs from tabletoolz.sql import T, D from sqlalchemy.orm import sessionmaker from sqlalchemy import create_engine from sqlalchemy.ext.automap import automap_base; . # Set-up the database engine = create_engine(&quot;sqlite:///tabletoolz/databases/Car_Database.db&quot;) Base = automap_base() Base.prepare(engine, reflect=True) # Assign the tables brands = Base.classes.Brands car_options = Base.classes.Car_Options car_vins = Base.classes.Car_Vins customer_ownership = Base.classes.Customer_Ownership dealers = Base.classes.Dealers manufacture_plant = Base.classes.Manufacture_Plant models = Base.classes.Models . In Python, libaries such as dfply makes function chaining possible through the use of pipes which is similar to R&#39;s dplyr. Tabletoolz_Sql borrows from dfply to achieve this method of data manipulation. . After loading your tables, . Important: every operation begins by turning a table into a statement. . df = car_options &gt;&gt; tbs.to_statement . . Tip: Statements can be printed in the SQL format. . df = car_options &gt;&gt; tbs.to_statement &gt;&gt; tbs.pprint . SELECT &#34;Car_Options&#34;.option_set_id, &#34;Car_Options&#34;.model_id, &#34;Car_Options&#34;.engine_id, &#34;Car_Options&#34;.transmission_id, &#34;Car_Options&#34;.chassis_id, &#34;Car_Options&#34;.premium_sound_id, &#34;Car_Options&#34;.color, &#34;Car_Options&#34;.option_set_price FROM &#34;Car_Options&#34; . A common data manipulation task . Let&#39;s say we want to compute the Mean of the model base prices. We can begin by turning the table into a statement. Group-by our column of interest and compute the Mean statistic. Finally, we can select the columns we want to output and turn it into a dataframe. . Mean = tbs.sql_func(&#39;avg&#39;) . # Mutate df = (models &gt;&gt; tbs.to_statement &gt;&gt; tbs.group_by([T.brand_id]) &gt;&gt; tbs.mutate(model_base_prices = Mean(T.model_base_price)) &gt;&gt; tbs.select([T.brand_id,&#39;model_base_prices&#39;]) &gt;&gt; tbs.head(num=5) &gt;&gt; tbs.to_pandas(engine)); df . brand_id model_base_prices . 0 1 | 25666.666667 | . 1 2 | 24666.666667 | . 2 3 | 14666.666667 | . 3 4 | 102500.000000 | . 4 5 | 32500.000000 | . Other data manipulation tasks . Arrange . Columns can be arranged either in ascending or descending order. . df = (car_options &gt;&gt; tbs.to_statement &gt;&gt; tbs.arrange([T.option_set_price, T.model_id, &quot;engine_id&quot;], ascending=True) &gt;&gt; tbs.head(num=5) &gt;&gt; tbs.to_pandas(engine)); df . option_set_id model_id engine_id transmission_id chassis_id premium_sound_id color option_set_price . 0 26 | 11 | 2 | 4 | 6 | NaN | Magenta | 200 | . 1 25 | 1 | 2 | 4 | 7 | NaN | Red | 900 | . 2 12 | 1 | 1 | 4 | 7 | NaN | Black | 1100 | . 3 3 | 6 | 1 | 4 | 7 | 5.0 | Green | 1100 | . 4 2 | 5 | 2 | 4 | 6 | 5.0 | Yellow | 1200 | . Renaming a column . df = (car_options &gt;&gt; tbs.to_statement &gt;&gt; tbs.rename(COLOUR=T.color, PRICE=T.option_set_price) &gt;&gt; tbs.head(num=5) &gt;&gt; tbs.to_pandas(engine)); df . option_set_id model_id engine_id transmission_id chassis_id premium_sound_id COLOUR PRICE . 0 1 | 4 | 2 | 4 | 6 | 5 | Blue | 2000 | . 1 2 | 5 | 2 | 4 | 6 | 5 | Yellow | 1200 | . 2 3 | 6 | 1 | 4 | 7 | 5 | Green | 1100 | . 3 4 | 15 | 1 | 4 | 7 | 5 | Red | 4000 | . 4 5 | 14 | 2 | 4 | 7 | 5 | Sky | 3000 | . Unite columns with a separator . df = (car_options &gt;&gt; tbs.to_statement &gt;&gt; tbs.unite([T.chassis_id, T.option_set_price, T.color], &quot;UNITED_COL&quot;, sep=&#39;_&#39;, remove=True) &gt;&gt; tbs.head(num=5) &gt;&gt; tbs.to_pandas(engine) ); df . option_set_id model_id engine_id transmission_id premium_sound_id UNITED_COL . 0 1 | 4 | 2 | 4 | 5 | 6_2000_Blue | . 1 2 | 5 | 2 | 4 | 5 | 6_1200_Yellow | . 2 3 | 6 | 1 | 4 | 5 | 7_1100_Green | . 3 4 | 15 | 1 | 4 | 5 | 7_4000_Red | . 4 5 | 14 | 2 | 4 | 5 | 7_3000_Sky | . Performing joins . Joins can be performed by specifying the tables that you want to join and the columns to join on. . Table1 = car_options &gt;&gt; tbs.to_statement Table2 = car_vins &gt;&gt; tbs.to_statement . # Columns to join on Cols = (Table1.c.model_id== Table2.c.model_id) &amp; (Table1.c.option_set_id == Table2.c.option_set_id) . df = (Table1 &gt;&gt; tbs.left_join(Table2, onclause=Cols) &gt;&gt; tbs.head(num=5) &gt;&gt; tbs.to_pandas(engine)); df . option_set_id model_id engine_id transmission_id chassis_id premium_sound_id color option_set_price vin model_id option_set_id manufactured_date manufactured_plant_id . 0 1 | 4 | 2 | 4 | 6 | 5 | Blue | 2000 | 5.0 | 4.0 | 1.0 | 2013-06-22 | 4.0 | . 1 2 | 5 | 2 | 4 | 6 | 5 | Yellow | 1200 | NaN | NaN | NaN | None | NaN | . 2 3 | 6 | 1 | 4 | 7 | 5 | Green | 1100 | NaN | NaN | NaN | None | NaN | . 3 4 | 15 | 1 | 4 | 7 | 5 | Red | 4000 | NaN | NaN | NaN | None | NaN | . 4 5 | 14 | 2 | 4 | 7 | 5 | Sky | 3000 | 2.0 | 14.0 | 5.0 | 2016-09-19 | 3.0 | . For more info on the library: 1 . 1. Visit the github.↩ .",
            "url": "https://dqniellew1.github.io/dqniellew/jupyter/data%20manipulation/projects/2020/06/05/How_to_use_Tabletoolz.html",
            "relUrl": "/jupyter/data%20manipulation/projects/2020/06/05/How_to_use_Tabletoolz.html",
            "date": " • Jun 5, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Daniel. I recently graduated from college with a bachelor’s degree in Data Science. I am currently looking for a job in a data-related position; Data Analyst, Data Scientist. That said, I am eager to apply data science to solve real-world problems. . This space will be used to write about my projects, experiences, and topics surrounding data science that interest me. . Timeline: . Winona State University (August, 2017 - May 2020) (student) GitHub | LinkedIn | [Email(mailto:dqniellew@gmail.com) | .",
          "url": "https://dqniellew1.github.io/dqniellew/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dqniellew1.github.io/dqniellew/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}